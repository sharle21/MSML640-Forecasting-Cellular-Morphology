{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfa8FtJm6GNmAQT+S7eTvd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0lr4tsU9Z7h",
        "outputId": "7d2074cf-d35e-4bd8-dce1-a4b9c12abea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries\n",
            "\n",
            "Mounting Google Drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted successfully.\n",
            "\n",
            "Cloning GitHub repository...\n",
            "/content\n",
            "Cloning into '2024_Chandrasekaran_NatureMethods_CPJUMP1'...\n",
            "remote: Enumerating objects: 6390, done.\u001b[K\n",
            "remote: Counting objects: 100% (2400/2400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1394/1394), done.\u001b[K\n",
            "remote: Total 6390 (delta 1031), reused 2217 (delta 982), pack-reused 3990 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6390/6390), 822.19 MiB | 29.70 MiB/s, done.\n",
            "Resolving deltas: 100% (1833/1833), done.\n",
            "Updating files: 100% (1757/1757), done.\n",
            "Downloading load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz (143 KB)\n",
            "Error downloading object: load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz (33782fc): Smudge error: Error downloading load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz (33782fca8602a7a0d7ec71aa6a093964ecbcb1adaf1967c63e8039cc51559ab4): batch response: This repository exceeded its LFS budget. The account responsible for the budget should increase it to restore access.\n",
            "\n",
            "Errors logged to /content/2024_Chandrasekaran_NatureMethods_CPJUMP1/.git/lfs/logs/20251022T041318.576592175.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "/content/2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
            "Current directory set to: /content/2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
            "\n",
            " Downloading profiles for ONE batch: 2020_12_02_CPJUMP1_2WeeksTimePoint\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_12_02_CPJUMP1_2WeeksTimePoint/ profiles/2020_12_02_CPJUMP1_2WeeksTimePoint --exclude *.sqlite\n",
            " Successfully downloaded profiles to profiles/2020_12_02_CPJUMP1_2WeeksTimePoint\n",
            "\n",
            " Starting Data Load & Metadata Preparation \n",
            " Perturbation details loaded \n",
            " Platemaps loaded \n",
            " Experiment/Timepoint data loaded \n",
            "Creating base metadata...\n",
            " Base 'meta' DataFrame created \n",
            "\n",
            " Loading Profiles Plate by Plate for Batch: 2020_12_02_CPJUMP1_2WeeksTimePoint \n",
            "  Processing plate: BR00117023\n",
            "    -> Merged 11640 wells.\n",
            "  Processing plate: BR00118039\n",
            "    -> Merged 13968 wells.\n",
            "  Processing plate: BR00117021\n",
            "    -> Merged 11640 wells.\n",
            "  Processing plate: BR00118050\n",
            "    -> Merged 11640 wells.\n",
            "  Processing plate: BR00118040\n",
            "    -> Merged 13968 wells.\n",
            "  Processing plate: BR00117022\n",
            "    -> Merged 11640 wells.\n",
            "  Processing plate: BR00117020\n",
            "    -> Merged 11520 wells.\n",
            "  Processing plate: BR00117006\n",
            "    -> Merged 11640 wells.\n",
            "\n",
            "Concatenating all processed plates\n",
            " COMPLETE MASTER DATAFRAME CREATED \n",
            "Total rows (from one batch): 97656\n",
            "\n",
            "Saving master DataFrame to Google Drive\n",
            " Checkpoint Saved to /content/drive/My Drive/CPJUMP1_master_data_BATCH_1.parquet \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "\n",
        "#Installing dependencies\n",
        "print(\"Installing required libraries\")\n",
        "!pip install -q pandas numpy scikit-learn plotly awscli openpyxl pyarrow\n",
        "\n",
        "print(\"\\nMounting Google Drive\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting drive: {e}\")\n",
        "\n",
        "#Cloning the official dataset\n",
        "print(\"\\nCloning GitHub repository...\")\n",
        "%cd /content/\n",
        "!rm -rf 2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
        "!git clone https://github.com/jump-cellpainting/2024_Chandrasekaran_NatureMethods_CPJUMP1.git\n",
        "\n",
        "try:\n",
        "    %cd /content/2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
        "    print(f\"Current directory set to: {os.getcwd()}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: git clone failed. Cannot change directory.\")\n",
        "    raise SystemExit(\"Stopping script due to clone failure.\")\n",
        "\n",
        "#Downloading profiles for 1 batch\n",
        "batch_to_download = \"2020_12_02_CPJUMP1_2WeeksTimePoint\"\n",
        "print(f\"\\n Downloading profiles for ONE batch: {batch_to_download}\")\n",
        "\n",
        "s3_path = f\"s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/{batch_to_download}/\"\n",
        "local_dir = f\"profiles/{batch_to_download}\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "aws_command = [\n",
        "    \"aws\", \"s3\", \"cp\", \"--no-sign-request\", \"--recursive\",\n",
        "    s3_path, local_dir, \"--exclude\", \"*.sqlite\"\n",
        "]\n",
        "try:\n",
        "    print(f\"Running command: {' '.join(aws_command)}\")\n",
        "    subprocess.run(aws_command, check=True, capture_output=True, text=True)\n",
        "    print(f\" Successfully downloaded profiles to {local_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n ERROR: Download failed.  {e}\")\n",
        "    raise SystemExit(\"Stopping script due to download failure.\")\n",
        "\n",
        "#Loading metadata\n",
        "print(\"\\n Starting Data Load & Metadata Preparation \")\n",
        "\n",
        "#Loading pertubation data\n",
        "folder_path = \"metadata/external_metadata\"\n",
        "pert_details = {}\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        try:\n",
        "            if filename.endswith(\".tsv\"):\n",
        "                pert_details[filename] = pd.read_csv(file_path, sep='\\t')\n",
        "            elif filename.endswith(\".xlsx\"):\n",
        "                pert_details[filename] = pd.read_excel(file_path)\n",
        "        except Exception: pass\n",
        "compound_df = pert_details[\"JUMP-Target-1_compound_metadata.tsv\"].rename(columns={'pert_iname': 'perturbation'})\n",
        "crispr_df = pert_details[\"JUMP-Target-1_crispr_metadata.tsv\"]\n",
        "orf_df = pert_details[\"JUMP-Target-1_orf_metadata.tsv\"]\n",
        "crispr_df['perturbation'] = crispr_df['gene']\n",
        "orf_df['perturbation'] = orf_df['gene']\n",
        "all_pert_details_df = pd.concat([compound_df, crispr_df, orf_df], ignore_index=True)\n",
        "print(\" Perturbation details loaded \")\n",
        "\n",
        "\n",
        "platemap_files = glob.glob(\"metadata/platemaps/*/platemap/*.txt\")\n",
        "all_platemaps = []\n",
        "\n",
        "for f in platemap_files:\n",
        "    map_name = os.path.basename(f).replace('.txt', '')\n",
        "    temp_df = pd.read_csv(f, sep='\\t')\n",
        "    temp_df['Plate_Map_Name'] = map_name\n",
        "    all_platemaps.append(temp_df)\n",
        "platemap_df = pd.concat(all_platemaps, ignore_index=True)\n",
        "platemap_df = platemap_df.rename(columns={'well': 'Metadata_Well', 'broad_sample': 'Metadata_Broad_Sample'})\n",
        "print(\" Platemaps loaded \")\n",
        "\n",
        "#Loading experimental data\n",
        "barcode_files = glob.glob(\"metadata/platemaps/*/barcode_platemap.csv\")\n",
        "all_experiments = []\n",
        "\n",
        "for f in barcode_files:\n",
        "    timepoint_name = f.split('/')[2]\n",
        "    temp_df = pd.read_csv(f)\n",
        "    temp_df['Timepoint'] = timepoint_name\n",
        "    all_experiments.append(temp_df)\n",
        "main_df = pd.concat(all_experiments, ignore_index=True)\n",
        "print(\" Experiment/Timepoint data loaded \")\n",
        "\n",
        "\n",
        "print(\"Creating base metadata...\")\n",
        "meta = pd.merge(main_df, platemap_df, on=\"Plate_Map_Name\")\n",
        "meta = pd.merge(meta, all_pert_details_df, left_on=\"Metadata_Broad_Sample\", right_on=\"broad_sample\")\n",
        "print(\" Base 'meta' DataFrame created \")\n",
        "\n",
        "\n",
        "\n",
        "#Loading profiles\n",
        "print(f\"\\n Loading Profiles Plate by Plate for Batch: {batch_to_download} \")\n",
        "search_path = f\"{local_dir}/**/*.csv\"\n",
        "all_csv_files = glob.glob(search_path, recursive=True)\n",
        "\n",
        "merged_plate_data = []\n",
        "\n",
        "for f in all_csv_files:\n",
        "    try:\n",
        "        barcode = f.split('/')[-2]\n",
        "        print(f\"  Processing plate: {barcode}\")\n",
        "\n",
        "        #Load one plate's profile data\n",
        "        profile_df_plate = pd.read_csv(f)\n",
        "        profile_df_plate['Assay_Plate_Barcode'] = barcode\n",
        "\n",
        "        #Filter the main metadata for JUST this plate\n",
        "        meta_plate = meta[meta['Assay_Plate_Barcode'] == barcode]\n",
        "\n",
        "        #Merge this single plate's profiles with its metadata\n",
        "        merged_df_plate = pd.merge(\n",
        "            meta_plate,\n",
        "            profile_df_plate,\n",
        "            left_on=[\"Assay_Plate_Barcode\", \"well_position\"],\n",
        "            right_on=[\"Assay_Plate_Barcode\", \"Metadata_Well\"]\n",
        "        )\n",
        "\n",
        "        #Append the merged result to list\n",
        "        merged_plate_data.append(merged_df_plate)\n",
        "\n",
        "        print(f\"    -> Merged {len(merged_df_plate)} wells.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    -> Skipping file {f}, error: {e}\")\n",
        "\n",
        "if not merged_plate_data:\n",
        "     print(\"\\n\\n--- CRITICAL ERROR ---\")\n",
        "     print(\"Failed to process any plates. No data to save.\")\n",
        "else:\n",
        "    print(\"\\nConcatenating all processed plates\")\n",
        "    #Combine the merged data from all plates\n",
        "    final_master_df = pd.concat(merged_plate_data, ignore_index=True)\n",
        "    print(\" COMPLETE MASTER DATAFRAME CREATED \")\n",
        "    print(f\"Total rows (from one batch): {len(final_master_df)}\")\n",
        "\n",
        "\n",
        "    print(\"\\nSaving master DataFrame to Google Drive\")\n",
        "    save_path = \"/content/drive/My Drive/CPJUMP1_master_data_BATCH_1.parquet\"\n",
        "    final_master_df.to_parquet(save_path)\n",
        "    print(f\" Checkpoint Saved to {save_path} \")\n"
      ]
    }
  ]
}