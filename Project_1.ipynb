{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuLkz2Gkn2ssS/vFiPBoGI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dJ5B-77X6BGB",
        "outputId": "638ae785-f6e3-4323-e77f-664c42baa4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries...\n",
            "\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted successfully.\n",
            "\n",
            "Cloning GitHub repository...\n",
            "Cloning into '2024_Chandrasekaran_NatureMethods_CPJUMP1'...\n",
            "remote: Enumerating objects: 6390, done.\u001b[K\n",
            "remote: Counting objects: 100% (2400/2400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1394/1394), done.\u001b[K\n",
            "remote: Total 6390 (delta 1031), reused 2217 (delta 982), pack-reused 3990 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6390/6390), 822.18 MiB | 31.39 MiB/s, done.\n",
            "Resolving deltas: 100% (1833/1833), done.\n",
            "Updating files: 100% (1757/1757), done.\n",
            "Downloading load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz (143 KB)\n",
            "Error downloading object: load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz (33782fc): Smudge error: Error downloading load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz (33782fca8602a7a0d7ec71aa6a093964ecbcb1adaf1967c63e8039cc51559ab4): batch response: This repository exceeded its LFS budget. The account responsible for the budget should increase it to restore access.\n",
            "\n",
            "Errors logged to /content/2024_Chandrasekaran_NatureMethods_CPJUMP1/2024_Chandrasekaran_NatureMethods_CPJUMP1/.git/lfs/logs/20251022T031931.326537691.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: load_data_csv/2020_11_04_CPJUMP1/BR00116991/load_data.csv.gz: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "/content/2024_Chandrasekaran_NatureMethods_CPJUMP1/2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
            "Current directory set to: /content/2024_Chandrasekaran_NatureMethods_CPJUMP1/2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
            "Found 7 batches to download: ['2020_11_04_CPJUMP1_DL' '2020_12_02_CPJUMP1_2WeeksTimePoint'\n",
            " '2020_11_18_CPJUMP1_TimepointDay1' '2020_11_04_CPJUMP1'\n",
            " '2020_12_07_CPJUMP1_4WeeksTimePoint' '2020_12_08_CPJUMP1_Bleaching'\n",
            " '2020_11_19_TimepointDay4']\n",
            "\n",
            "--- Downloading profiles for batch: 2020_11_04_CPJUMP1_DL ---\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_11_04_CPJUMP1_DL/ profiles/2020_11_04_CPJUMP1_DL --exclude *.sqlite\n",
            "--- Successfully downloaded profiles to profiles/2020_11_04_CPJUMP1_DL ---\n",
            "\n",
            "--- Downloading profiles for batch: 2020_12_02_CPJUMP1_2WeeksTimePoint ---\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_12_02_CPJUMP1_2WeeksTimePoint/ profiles/2020_12_02_CPJUMP1_2WeeksTimePoint --exclude *.sqlite\n",
            "--- Successfully downloaded profiles to profiles/2020_12_02_CPJUMP1_2WeeksTimePoint ---\n",
            "\n",
            "--- Downloading profiles for batch: 2020_11_18_CPJUMP1_TimepointDay1 ---\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_11_18_CPJUMP1_TimepointDay1/ profiles/2020_11_18_CPJUMP1_TimepointDay1 --exclude *.sqlite\n",
            "--- Successfully downloaded profiles to profiles/2020_11_18_CPJUMP1_TimepointDay1 ---\n",
            "\n",
            "--- Downloading profiles for batch: 2020_11_04_CPJUMP1 ---\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_11_04_CPJUMP1/ profiles/2020_11_04_CPJUMP1 --exclude *.sqlite\n",
            "--- Successfully downloaded profiles to profiles/2020_11_04_CPJUMP1 ---\n",
            "\n",
            "--- Downloading profiles for batch: 2020_12_07_CPJUMP1_4WeeksTimePoint ---\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_12_07_CPJUMP1_4WeeksTimePoint/ profiles/2020_12_07_CPJUMP1_4WeeksTimePoint --exclude *.sqlite\n",
            "--- Successfully downloaded profiles to profiles/2020_12_07_CPJUMP1_4WeeksTimePoint ---\n",
            "\n",
            "--- Downloading profiles for batch: 2020_12_08_CPJUMP1_Bleaching ---\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_12_08_CPJUMP1_Bleaching/ profiles/2020_12_08_CPJUMP1_Bleaching --exclude *.sqlite\n",
            "--- Successfully downloaded profiles to profiles/2020_12_08_CPJUMP1_Bleaching ---\n",
            "\n",
            "--- Downloading profiles for batch: 2020_11_19_TimepointDay4 ---\n",
            "Running command: aws s3 cp --no-sign-request --recursive s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/2020_11_19_TimepointDay4/ profiles/2020_11_19_TimepointDay4 --exclude *.sqlite\n",
            "--- Successfully downloaded profiles to profiles/2020_11_19_TimepointDay4 ---\n",
            "\n",
            "--- Starting Data Load & Merge Process ---\n",
            "--- Perturbation details loaded ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'map_' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3540020107.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mall_platemaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplatemap_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmap_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Plate_Map_Name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'map_' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "\n",
        "# ======================================================================\n",
        "# Cell 1: Install All Dependencies & Mount Drive\n",
        "# ======================================================================\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install -q pandas numpy torch torchvision transformers timm pillow tqdm scikit-learn umap-learn plotly awscli openpyxl pyarrow\n",
        "\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting drive: {e}\")\n",
        "\n",
        "# ======================================================================\n",
        "# Cell 2: Clone Repo\n",
        "# ======================================================================\n",
        "print(\"\\nCloning GitHub repository...\")\n",
        "# Clean up previous clone if it exists, just in case\n",
        "!rm -rf 2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
        "!git clone https://github.com/jump-cellpainting/2024_Chandrasekaran_NatureMethods_CPJUMP1.git\n",
        "\n",
        "try:\n",
        "    %cd 2024_Chandrasekaran_NatureMethods_CPJUMP1\n",
        "    print(f\"Current directory set to: {os.getcwd()}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: git clone failed. Cannot change directory.\")\n",
        "    # Stop the script if cloning failed\n",
        "    raise SystemExit(\"Stopping script due to clone failure.\")\n",
        "\n",
        "# ======================================================================\n",
        "# Cell 3: Find Batch Names (A quick pre-load)\n",
        "# ======================================================================\n",
        "barcode_files_temp = glob.glob(\"metadata/platemaps/*/barcode_platemap.csv\")\n",
        "all_experiments_temp = []\n",
        "for f in barcode_files_temp:\n",
        "    timepoint_name = f.split('/')[2]\n",
        "    temp_df = pd.read_csv(f)\n",
        "    temp_df['Timepoint'] = timepoint_name\n",
        "    all_experiments_temp.append(temp_df)\n",
        "main_df_temp = pd.concat(all_experiments_temp, ignore_index=True)\n",
        "\n",
        "# *** THIS IS THE FIX (Part 1) ***\n",
        "# Get ALL batch names, not just the first one\n",
        "all_batch_names = main_df_temp['Timepoint'].unique()\n",
        "print(f\"Found {len(all_batch_names)} batches to download: {all_batch_names}\")\n",
        "\n",
        "# ======================================================================\n",
        "# Cell 4: Download Pre-Computed Profiles (Now loops)\n",
        "# ======================================================================\n",
        "\n",
        "# *** THIS IS THE FIX (Part 2) ***\n",
        "# Loop through every batch and download its profiles\n",
        "for batch_to_download in all_batch_names:\n",
        "    print(f\"\\n--- Downloading profiles for batch: {batch_to_download} ---\")\n",
        "    s3_path = f\"s3://cellpainting-gallery/cpg0000-jump-pilot/source_4/workspace/backend/{batch_to_download}/\"\n",
        "    local_dir = f\"profiles/{batch_to_download}\"\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "    # Download command (excluding giant .sqlite files)\n",
        "    aws_command = [\n",
        "        \"aws\", \"s3\", \"cp\", \"--no-sign-request\", \"--recursive\",\n",
        "        s3_path, local_dir, \"--exclude\", \"*.sqlite\"\n",
        "    ]\n",
        "    try:\n",
        "        print(f\"Running command: {' '.join(aws_command)}\")\n",
        "        subprocess.run(aws_command, check=True)\n",
        "        print(f\"--- Successfully downloaded profiles to {local_dir} ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- WARNING: Download failed for batch {batch_to_download}. --- {e}\")\n",
        "        print(\"This is OK, the batch might be empty. Continuing...\")\n",
        "\n",
        "# ======================================================================\n",
        "# Cell 5: Load, Merge, and Save Master Checkpoint\n",
        "# ======================================================================\n",
        "print(\"\\n--- Starting Data Load & Merge Process ---\")\n",
        "\n",
        "# --- Load PERTURBATION DETAILS (Unchanged) ---\n",
        "folder_path = \"metadata/external_metadata\"\n",
        "pert_details = {}\n",
        "for filename in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        try:\n",
        "            if filename.endswith(\".tsv\"):\n",
        "                pert_details[filename] = pd.read_csv(file_path, sep='\\t')\n",
        "            elif filename.endswith(\".xlsx\"):\n",
        "                pert_details[filename] = pd.read_excel(file_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "compound_df = pert_details[\"JUMP-Target-1_compound_metadata.tsv\"].rename(columns={'pert_iname': 'perturbation'})\n",
        "crispr_df = pert_details[\"JUMP-Target-1_crispr_metadata.tsv\"]\n",
        "orf_df = pert_details[\"JUMP-Target-1_orf_metadata.tsv\"]\n",
        "crispr_df['perturbation'] = crispr_df['gene']\n",
        "orf_df['perturbation'] = orf_df['gene']\n",
        "all_pert_details_df = pd.concat([compound_df, crispr_df, orf_df], ignore_index=True)\n",
        "print(\"--- Perturbation details loaded ---\")\n",
        "\n",
        "# --- Load PLATEMAPS (Unchanged) ---\n",
        "platemap_files = glob.glob(\"metadata/platemaps/*/platemap/*.txt\")\n",
        "all_platemaps = []\n",
        "for f in platemap_files:\n",
        "    map_.name = os.path.basename(f).replace('.txt', '')\n",
        "    temp_df = pd.read_csv(f, sep='\\t')\n",
        "    temp_df['Plate_Map_Name'] = map_name\n",
        "    all_platemaps.append(temp_df)\n",
        "platemap_df = pd.concat(all_platemaps, ignore_index=True)\n",
        "platemap_df = platemap_df.rename(columns={'well': 'Metadata_Well', 'broad_sample': 'Metadata_Broad_Sample'})\n",
        "print(\"--- Platemaps loaded ---\")\n",
        "\n",
        "# --- Load EXPERIMENT DATA (Timepoints) (Unchanged) ---\n",
        "main_df = main_df_temp # Use the one we loaded in Cell 3\n",
        "print(\"--- Experiment/Timepoint data loaded ---\")\n",
        "\n",
        "# --- Load DOWNLOADED PROFILES ---\n",
        "# *** THIS IS THE FIX (Part 3) ***\n",
        "# Search *all* subdirectories in 'profiles/' for .csv files\n",
        "search_path = f\"profiles/**/*.csv\"\n",
        "all_csv_files = glob.glob(search_path, recursive=True)\n",
        "\n",
        "# Check if we found any files at all\n",
        "if not all_csv_files:\n",
        "    print(\"\\n\\n--- CRITICAL ERROR ---\")\n",
        "    print(\"No .csv files were found in any downloaded batch folder.\")\n",
        "    print(\"Please check the 'aws' command output.\")\n",
        "    raise SystemExit(\"Stopping script: No data to load.\")\n",
        "\n",
        "print(f\"\\nFound {len(all_csv_files)} total profile files to load.\")\n",
        "all_profiles_list = []\n",
        "for f in all_csv_files:\n",
        "    try:\n",
        "        temp_df = pd.read_csv(f)\n",
        "        barcode = f.split('/')[-2] # Get plate barcode from folder name\n",
        "        temp_df['Assay_Plate_Barcode'] = barcode\n",
        "        all_profiles_list.append(temp_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {f}, error: {e}\")\n",
        "\n",
        "# This check prevents the ValueError\n",
        "if not all_profiles_list:\n",
        "    print(\"\\n\\n--- CRITICAL ERROR ---\")\n",
        "    print(\"Failed to load any of the found .csv files.\")\n",
        "    raise SystemExit(\"Stopping script: No data to concatenate.\")\n",
        "\n",
        "all_profiles_df = pd.concat(all_profiles_list, ignore_index=True)\n",
        "print(f\"--- Downloaded profiles loaded ---\")\n",
        "\n",
        "# --- MERGE EVERYTHING ---\n",
        "print(\"Merging...\")\n",
        "meta = pd.merge(main_df, platemap_df, on=\"Plate_Map_Name\")\n",
        "meta = pd.merge(meta, all_pert_details_df, left_on=\"Metadata_Broad_Sample\", right_on=\"broad_sample\")\n",
        "final_master_df = pd.merge(meta, all_profiles_df, left_on=[\"Assay_Plate_Barcode\", \"well_position\"], right_on=[\"Assay_Plate_Barcode\", \"Metadata_Well\"])\n",
        "print(\"--- ðŸŽ‰ COMPLETE MASTER DATAFRAME CREATED ---\")\n",
        "print(f\"Total rows: {len(final_master_df)}\")\n",
        "\n",
        "# --- SAVE CHECKPOINT ---\n",
        "print(\"\\nSaving master DataFrame to Google Drive...\")\n",
        "save_path = \"/content/drive/My Drive/CPJUMP1_master_data.parquet\"\n",
        "final_master_df.to_parquet(save_path)\n",
        "print(f\"--- ðŸŽ‰ Checkpoint Saved to {save_path} ---\")\n",
        "print(\"You can now open a new notebook and run the analysis cells.\")"
      ]
    }
  ]
}